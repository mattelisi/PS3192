---
title: "Advanced R programming --- worksheet"
subtitle: "PS3192 25-26"
author: "Matteo Lisi"
format:
  html:
    toc: true
    code-tools: true     # run/copy buttons
    code-fold: show
execute:
  echo: true
  warning: false
  message: false
---


## Activity 1 — Explanation vs. prediction: two perspective on liner regression

This dataset contains the annual earnings (in $), the heights (in inches) and the sex (male/female) of a random sample (N=1192) of adult Americans, surveyed in the 1990.

Download the file `earnings.csv` from the Moodle page, copy it in your working directory, and run the following code:

```{r, eval=FALSE}
earnings <- read.csv("earnings.csv")
```

Alternatively, you can directly import the data from a URL

```{r}
earnings <- read.csv("https://mlisi.xyz/PS3192/datasets/earnings.csv")

```


The command `str()` comes handy to examine the nature of objects in the working space

```{r}
str(earnings)
```

### A scientist/statistician perspective

Suppose we want to sue this data to address a question of whether people's height _causally_ influence their salary.

A first approach could be to fit the linear regression model:

```{r}
m <- lm(earn ~ height, data = earnings)
summary(m)
```

Which reveals a significant effect of `r round(coef(m)[2])` extra dollars per year for each additional in of height.

At this point the statistician gets suspicious — is this a _real_ effect?

One possible explanation is that this is _confounded_ by sex. Simply put, it is known that there are gender disparities in salary (and these were likely even larger in 1990), and that men tend to be taller (on average) than women. So this may be just a _spurious_ correlation. This is a common 'confounding by lurkign third variable' situation, which is summarised by the following causal graph[^dag]


[^dag]: The correct technical name for this type of representations is a DAG, acronym of "directed acyclic graph". Nodes represent variables, and arrows causal relationships.


```{r}
#| echo: FALSE
#| fig-height: 4.5
#| fig-width: 6
#| fig-align: center


library(ggplot2)
library(ggdag)
library(dagitty)

coord_dag <- list(x = c(X = -1, Y = 1, Z = 0)*1.5,
                  y = c(X = -1, Y = -1, Z = 1))

dag <- dagify(X ~ Z,
               Y ~ Z,
               Y~X,
               labels = c(
                 X = "Height",
                 Y = "Salary",
                 Z = "Sex"),
               coords = coord_dag)

dag %>% 
  tidy_dagitty(layout = "nicely") %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend))+
  geom_dag_point(colour = 'white', size = 40) +
  geom_dag_text(aes(label=label), colour = 'black',family="URWGothic", size=7) +
  geom_dag_edges(start_cap = ggraph::circle(14, 'mm'),
                 end_cap = ggraph::circle(14, 'mm')) +
  theme_dag() +
  coord_equal(xlim=c(-2,2),
              ylim=c(-1,1))
```

Indeed if we visualise the data we can see that the sex seems to be a major confounding factor:

```{r}
#| fig-height: 4
#| fig-width: 10
#| fig-align: center

# prepare plotting window
par(mfrow=c(1,3))

# scatterplot. first we plot males as blue dots
plot(earnings$height[earnings$female==0], 
     earnings$earn[earnings$female==0], 
     col="blue",cex=0.5, 
     ylab="annual earnings [$]",
     xlab="height [inches]",
     xlim=range(earnings$height))

# add females as red dots
points(earnings$height[earnings$female==1], 
       earnings$earn[earnings$female==1], 
       col="red",cex=0.5)

# add regression line
abline(lm(earn~height, earnings),lwd=2)


# for X label in boxplot
earnings$sex <- ifelse(earnings$female==1,"female","male")

# boxplots
boxplot(height ~ sex, earnings, notch=T, xlab="", ylab="height [inches]",col=c(rgb(1,0,0,0.5),rgb(0,0,1,0.5)))

boxplot(earn ~ sex, earnings, notch=T, xlab="", ylab="annual earnings [$]",col=c(rgb(1,0,0,0.5),rgb(0,0,1,0.5)))


```


One way to check this is to adjust our estimate of he effect of `height` by including sex in our model

```{r}
m2 <- lm(earn ~ height + female, data = earnings)
summary(m2)
```

(Note that here `female` is a so-called "dummy" variable, that is a variable use to encode a categorical label by means of 0 and 1 values. Specifically, we have that `female`=1 when the datapoint correspond to a female respondent, and `female`=0 otherwise.)

Our new model still reveal some effect of height, but this is substantially smaller, and a very large effect of sex, where women earn about 9 thousands dollar less than men per year on average. 


::: callout-tip

#### Linear regression recap

In linear regression we model a variable (_outcome_) as a function of one or more _predictor_ variable

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$
where $y$ is the outcome (sometime referred to as "dependent" variable), $x$ is the predictor, and $\beta_0$ and $\beta_1$ the intercept and slope coefficients, respectively. $\beta_0$ tells us the expected value of $y$ when $x=0$ and $\beta_1$ the expected increase in $y$ for a unitary increase (+1) in $x$.
$\epsilon$ indicates the residual or error terms, corresponding to the variability in the data that is not explained by our model. The key assumptions in linear regression (and all linear models, including ANOVA, t-test, etc.) are that these errors are _normally_ distributed and they they are i.i.d. (independent and identically distributed).

You can sometime see the model also notates as

$$\hat y_i = \beta_0 + \beta_1 x_i$$
Note that here we have removed the error term, and the dependent variable has a little "hat" symbol $\hat \,$. This is standard notation to indicate that to the left of the equal sign we are now indicating the expected or predicted values of the regressions. The differences between the predicted/expected values and the actual data are the residuals, that is we can write $\epsilon_i = y_i - \hat y_i$.

Formally, the central assumption of linear regression can be formally expressed as $\epsilon \sim \mathcal{N}(0, \sigma_{\epsilon})$, which reads as "the residuals of regression have a normal distribution with mean 0 and standard deviation of $\sigma_{\epsilon}$".

We can add more than one predictors , e.g.
$$y_i = \beta_0 + \beta_1 x_i + \beta_3 z_i + \epsilon_i$$
For example, our model `m2` above. which has 2 predictors, could be written formally as

$$\text{earnings}_i = \beta_0 + \beta_1 \, \text{height}_i + \beta_3  \, \text{female}_i + \epsilon_i$$
Essentially, the dependent variable is modeled as a weighted sum of the two predictors, plus a constant term (the intercept $\beta_0$).

As an **exercise**, try to apply the regression formula above using the values of coefficients from the fitted model to compute  the predicted/expected salary of:

- a male, with `height`=70 inches
- a female, with `height`=80 inches

You can extract the precise values using the function `coef()`

```{r}
coef(m2)
```


:::


### A data scientist perspective

A data scientist might work for a company that wants to use this data to offer personal styling packages priced relative to clients’ income. That is, we are not interested in _why_salary differs, we just want a model that makes good predictions.

The data scientist consider that perhaps we can make the model even more accurate by adding an interaction term:

```{r}
m3 <- lm(earn ~ height * female, data = earnings)
```

But how do we know which model will performs better in new data?

One standard approach in machine learning is to split the dataset in _train_ and _test_ data. Let's say we want to keep aside 30% of the data as our training set.

```{r}
# number of observations in the dataset
n <- nrow(earnings)

# row-indices of 30% randomly selected datapoints
test_idx <- sample(n, size = round(0.3 * n))

# split in test and training data
test_data  <- earnings[test_idx, ]
train_data <- earnings[-test_idx, ]
```

Now let's fit again our models `m2` adn `m3` but using only the training data

```{r}
m2_train <- lm(earn ~ height + female, data = train_data)
m3_train <- lm(earn ~ height * female, data = train_data)
```

Now let's evaluate their predictive performance on the training data. We will use the mean squared error (MSE) as an error metric:

$$\text{MSE} = \frac{\sum_{i=1}^n (y_i - \hat y_i)^2}{n}$$

```{r}
# for model m2
pred_m2 <- predict(m2_train, newdata = test_data)
MSE_m2 <- mean((test_data$earn - pred_m2)^2)

# for model m3
pred_m3 <- predict(m3_train, newdata = test_data)
MSE_m3 <- mean((test_data$earn - pred_m3)^2)
```

Results:

```{r}
cat(sprintf(" MSE model m2: %.2f\n MSE model m3: %.2f", MSE_m2, MSE_m3))
```


One concern, however, is that the results observed might be specific to this particular split of observations in train and test data. In other words, would we find the same results if a different subset of observations went in training and test sets?

A standard approach to deal with this is repeating the analysis with different splits of training and test sets, and then average the results. In order to do this, is helpful to get familiar with **control flow statements**, a fundamental but powerful type of programming structure. This allows to automatise complex tasks involving multiple steps and branching logic. When coding complex projects, it is also often helpful to build our own **custom functions**, especially when the same set of operations may be needed in multiple places within a project. The two boxes below illustrate these conceps, and you can find a more systematic introduction in the Chapter 1 of the textbook[^bookref]

[^bookref]: Clarke, A., & Lisi, M. (2025). Statistics for psychology using R: A linear models perspective. McGraw-Hill Education (UK). [ISBN 9780335252626](https://www.mheducation.co.uk/statistics-for-psychology-using-r-a-linear-models-perspective-9780335252626-emea-group). The book is available in the RHUL Library.


::: callout-tip

#### Control flow `for` & `if` statements

Some of the most powerful tools in computer coding are **for loops** and **if statements**. These allow us to repeat the same operations over different values, or ask a question about some variables and run code only if certain conditions are met. 

We will start with `for` loops. The basic structure is illustrated by the example below:

```{r}

for (i in c(5, 2, 7)) {
  
  print( 3*i -2 )
  
}
```

What is going on here? The `for (i in c(5, 2, 7))` part of the loop creates a new variable called `i` and initially sets it to 5. It runs the code in between the parenthesis `{}` using this value of `i`. It then sets `i` to the next value (2) and repeats. Finally, it runs the code one more with `i=7`. This can be very powerful! In our simple example here, the for loop doesn't save much time over writing out the `print()` statement three times (one for 5, 2 and 7), but imagine if we wanted to run `for(i in 1:1000)`! Similarly, the code in between the `{}` is quite short in this example, but in practise we often use more complex series of operations. 

On the other hand, `if` statement allows us to run a block of code *if* some condition is true. 

```{r}
n <- 7

if (n > 5) 
{
  print("x is greater than 5!")
} 
```

We can combine these with an `else` statement that tells R what to do if the conditions of the if statement are not true. For example:

```{r}
#| echo: false


if (n > 5) 
{
  print("x is greater than 5!")
  
} else {
  
 print("x is NOT greater than 5!")
  
}
```


:::



::: callout-tip

#### Custom functions

We can extend R by writing our own functions. Here is an example of a function that 'center' a variable; that is it subtract its mean, so that as a result the centered variable values will have a mean of zero. (This is often useful when interpreting a multiple regression model with interactions.)

Here is one way to write such a function:

```{r}
center <- function(x) {
  # subtract the mean from each value
  x_centered <- x - mean(x)
  
  return(x_centered)
}
```


Notice the general form for defining a function in R. The name for our new function comes first, followed by `<- function(  )`. Between the `( )` we list the values we want to pass to the function. Finally, we have a pair of `{ }` parenthesis which contain all of the code we wish to run when the function is called. The only new concept here is the `return()` function that passes the answer of our function back out to our R environment. 

Once we have executed the code above, the function is now available in R workspace (if you run the command `ls()` you should see a list of all objects in the workspace, and that should include something called `"center"`).

We can now use our function:

```{r}
# create a variable
v <- c(4,2,3,5,6,7,8,9,2,3,4,5)

# check it's mean
mean(v)

# use our function to center
v_centered <- center(v)

# the mean should now be zero (or approximately zero)
mean(v_centered)


```

(Note that the mean of the centered varial is not exacly zero, but something like `2.960403e-16`. This is scientific notation for $2.9\times 10^{-16}$ which is a very small number. It is not exactly zero because computers use something something called "floating-point arithmetic" to represent and perform calculations on real numbers, which introduces small rounding errors because most decimal fractions can't be perfectly represented).

:::


#### Putting it all together: cross-validation


Having covered this background, we can performa what machine learning folks call a 10-fold crossvalidation procedure:

First, it can be useful to write a custom function that compute the MSE given model and a formula syntax (note that you can pass the model formula to `lm()` as a character string).

```{r}
# function to compute test-set MSE -----------------------------

compute_MSE <- function(formula, train_data, test_data) {
  
  # fit the model on the training data
  model <- lm(formula, data = train_data)
  
  # predict outcomes for the test data
  pred <- predict(model, newdata = test_data)
  
  # compute mean squared error
  MSE <- mean((test_data$earn - pred)^2)
  
  return(MSE)
}

```

We can check that using this in the split we have used above return the same values:

```{r}
compute_MSE(earn ~ height + female, train_data, test_data)
compute_MSE(earn ~ height * female, train_data, test_data)

```


Finally, we can use this function in a `for` loop, and compute the predictive performance of our two models:

```{r}
# 10-fold cross-validation ------------------------------------

set.seed(123)  # for reproducibility

n <- nrow(earnings)
K <- 10

# assign each observation to one of K folds
fold_id <- rep(1:K, length.out = n)
fold_id <- sample(fold_id)

# storage for test MSEs
MSE_m2 <- numeric(K)
MSE_m3 <- numeric(K)

for (k in 1:K) {
  
  # split data into training and test sets
  test_idx  <- which(fold_id == k)
  train_idx <- which(fold_id != k)
  
  train_data <- earnings[train_idx, ]
  test_data  <- earnings[test_idx, ]
  
  # compute test MSE using our custom function
  MSE_m2[k] <- compute_MSE(earn ~ height + female,
                           train_data, test_data)
  
  MSE_m3[k] <- compute_MSE(earn ~ height * female,
                           train_data, test_data)
}

# average performance across folds
mean_MSE_m2 <- mean(MSE_m2)
mean_MSE_m3 <- mean(MSE_m3)

cat(sprintf("Average test MSE over %d folds:\n", K))
cat(sprintf("  model m2 (height + female): %.2f\n", mean_MSE_m2))
cat(sprintf("  model m3 (height * female): %.2f\n", mean_MSE_m3))

```

## Activity 2 — Predicting California house prices

In this activity you will work with a real-world housing dataset and focus explicitly on prediction rather than explanation.

Your goal is to build a model that predicts the median house value (`medv`) as accurately as possible for new, unseen data.

**Instructions**

1. Load the training data california_housing_train.csv from Moodle.
2. Fit a linear regression model to predict medv using a subset of the available predictors.
3. You are encouraged to experiment with:
    - Different combinations of predictors
    - Interaction terms
    - Non-linear terms (e.g. squared predictors)
4. To decide which model generalizes best, you may want to use hold-out validation or cross-validation on the training data.
_No code is provided for this step — designing and implementing it is part of the exercise._
5. Once you have selected a final model, use it to generate predictions for the test set.
6. Save your predictions (see below for an example).

In the next workshop, a test set with the true values will be released, and we will compare predictive performance across models.

#### Example

The code below illustrates a very simple baseline model using only a few predictors.
This is not expected to be optimal.

```{r}
# Load the training data
train <- read.csv("california_housing_train.csv")

# Inspect the variables
str(train)

```


```{r}
# Fit a simple linear regression model
# (you are encouraged to improve on this)
model_lm <- lm(medv ~ rm + lstat + ptratio + nox, data = train)

# Inspect model output
summary(model_lm)
```

**Predicting on the test set**

The file `california_housing_test_nomedv.csv` (available on Moodle) contains the same predictors as the training data,
but does not include `medv`.

```{r}
# Load the test data (no medv column)
test <- read.csv("california_housing_test_nomedv.csv")

# Generate predictions for the test set
pred_medv <- predict(model_lm, newdata = test)

# Check the first few predictions
head(pred_medv)
```


**Saving your predictions for submission**

Save your predictions as a CSV file with one column only, named `pred_medv`.

```{r}
# Create a data frame for submission
submission <- data.frame(pred_medv = pred_medv)

# Export to CSV
write.csv(submission,
          file = "california_housing_predictions.csv",
          row.names = FALSE)

```

This is optional, but if you send me this file via email (matteo.lisi [at] rhul.ac.uk) I will evaluate the predictions and make a ranking list for next time (please use **"california data exercise PS3192"** in the subject line). In the next session we will in any case discuss this example further.



