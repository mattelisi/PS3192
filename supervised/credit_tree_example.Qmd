---
title: "Decision tree â€” worked example"
subtitle: "PS3192 25-26"
author: "Matteo Lisi"
format:
  html:
    toc: true
    code-tools: true     # run/copy buttons
    code-fold: show
execute:
  echo: true
  warning: false
  message: false
---


## Setup

Load all necessary libraries

```{r}
library(rpart)
library(caret)
```


The dataset is 

```{r}
data(GermanCredit)
str(GermanCredit)
```
Essentially our goal is to predict the class of credit worthiness (`good` vs `bad`), and the predictors relates to several attributes in the dataset such age, acount status, credit hostory etc. You can have some more background information about the dataset from the help by typing `?GermanCredit` in the console[^datainfo].

[^datainfo]: Mode detailed information about the dataset can be found at this [link](https://archive.ics.uci.edu/dataset/522/south+german+credit) and in this [report](https://www1.beuth-hochschule.de/FB_II/reports/Report-2019-004.pdf). 

## Estimating a decision tree with default settings

As we saw in the workshop, using the default settigns of `rpart` will give us a decisiont tree pruned to a reasonable expect. Here we are using all available variables as potential predictors (by using the dot `.` in the formula `Class  ~ .`) and we only have slightly increased the number of cross-validation iteration to 250.

```{r}
#| fig-height: 8
#| fig-width: 16
#| fig-align: center
#| echo: TRUE

# estimate decision tree
credit_tree <- rpart(Class  ~ ., data = GermanCredit, xval=250)

# make a plot
plot(credit_tree)
text(credit_tree, use.n = TRUE, cex=0.9)
```

From the plot, we can see that the tree has 18 "leafs" (terminal nodes) and 17 splits.

### Default settings

The default parameters that we have used are the ones indicated in the help page of `part` control settings, which you can find either by typing `?rpart.control` in the command line, or alternatively simply `?rpart` and then following the link to `rpart.control` in the 'Arguments' part of the help page. 

You will see that under **Usage** the page indicates:

```
rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, 
              maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,
              surrogatestyle = 0, maxdepth = 30, ...)
```

Essentially the values listed there are the "default" value. This tells us for example that by default `rpart` won't attempt to split a node unless the number of observations at that node are at least `minsplit = 20`.

In addition to `minsplit` two key arguments are:

- **`cp`** is the complexity parameter. It defines the minimum amount of improvement in the overall fit of the model for any node: any split that does not decrease the overall "cost" of the tree by a factor of `cp` will note be attempted. Note that this refers to the _overall_ cost of the model, which depends on the proportion of misclassified observations (thus not the local impurity measure that is minimized at each node during the initial part of model fitting). 

- **`xval`** is the number of cross-validation iterations. The cross-validation is done internally by `rpart` to decide the optimal level of complexity

### Cross-validation results

We can visualise the cross-validation results as

```{r}
printcp(credit_tree)
```

The table report cross-validation results for trees of increasing complexity. 

In class we defined the values in the table as follow:
- `CP` is the complexity parameter
- `nsplit` the number of split of the tree
- `rel error` is the error in the training set
- `xerror` is the error in the test sets of the crossvalidation
- `xstd` is the standard deviation of the test error across the cross-validation folds

Strictly speaking, `CP` should be interpreted as the smallest complexity penalty at which the tree in each row of the table becomes optimal under cost-complexity pruning. Let's unpack this, and define more formally in the optional box below.

Intuitively, the larger is the complexity parameter, the more we are penalising complexity when pruning the tree. You can think of this as a dial that says "How much extra error am I willing to tolerate to remove splits and make the tree simpler?". If `CP` is quite large (say `CP`=0.04) if means we accepts splits only if they reduce the error by a large amount. Based on the table above, if we were to set the penalty threshold to 0.04, we would conclude that the best model is the tree with no splits (since the table indicates this is optimal up until values of `CP` of 0.044444).

Plotting the cross-validation results display nicely how the cross-validation error increases for overly complex trees. Note that I am transforming the cost into "raw" misclassification units.

```{r}
#| fig-height: 6
#| fig-width: 6
#| fig-align: center
#| echo: TRUE

# extract table
cp_data <- as.data.frame(credit_tree$cptable)
colnames(cp_data) <- c("cp", "nsplit", "rel_error", "xerror", "xstd")

# Compute number of terminal nodes
cp_data$size <- cp_data$nsplit + 1
cp_data$xstd <- cp_data$xstd

# The errors are normalised relative to the model with no-splits
# to put them in the natural scale we need to multiply by the fraction 
# of errors in the no-split model, that is the same as the error we 
# would make by always simply predicting the majority class:
table(GermanCredit$Class)

# we can re-scale the errors as follow
cp_data[,c("rel_error", "xerror", "xstd")] <- cp_data[,c("rel_error", "xerror", "xstd")] * 300/(300 + 700)

# Plot misclassification error vs. number of terminal nodes
ggplot(cp_data, aes(x = size)) +
  geom_line(aes(y = rel_error, color = "Training"), linewidth = 1) +
  geom_line(aes(y = xerror, color = "Cross-validation"), linewidth = 1) +
  geom_errorbar(aes(y = xerror, ymin=xerror-xstd, ymax=xerror+xstd), color="blue", width=0.1, linewidth = 1) +
  geom_point(aes(x = size[which.min(xerror)], y = min(xerror)), color = "purple", size = 8, shape = 1) +
  labs(
    x = "Number of terminal nodes",
    y = "Fraction of misclassifications",
  ) +
  scale_color_manual(values = c("Training" = "red", "Cross-validation" = "blue"), name="") +
  theme_minimal()

```


::: callout-tip

### The cost function of decision trees

In the classical formulation of decision trees[^breiman] the cost is defined as follow.

[^breiman]: L. Breiman, J.H. Friedman, R.A. Olshen, , and C.J Stone. _Classification and Regression Trees._ Wadsworth, Belmont, Ca, 1983.

Say we have a tree with $k$ terminal nodes (leafs), $T_1, T_2, \ldots, T_k$. (Note: the tree with $k$ leafs has $k-1$ splits.)

The error of the tree (fraction of misclassified observations in our case) is called _**risk**_ in jargon, and when it is combined across all leafs, it gives us the overall _risk_ of the tree, notated as $R(T)$. This is a measure of error that is not _penalised_ for complexity: it is minimised by a tree complex enough to classify all observations perfectly.

The complexity penalty is introduced by defining the cost of the tree $T$ as follow:

$$
R_{\alpha}(T) = R(T)+\alpha \left| T \right|
$$
 $\left| T \right|$ is the number of terminal nodes or leafs (so it would be $\left| T \right| = k$ as we have $k$ terminal nodes). 
 
Note that in this formulation the $alpha$ is expressed in the same units of measurements of $R(T)$, which in our case would the proportion or _rate_ of misclassification.

`rpart` implementation adopts a slightly different formulation: the error that appear in the table as `rel error` is defined as 

$$\tilde{R}(T) = \frac{R(T)}{R(T_0)}$$

Essentially by dividing by $R(T_0)$ we are normalising the cost, so that the cost for the model with 0 splits will be 1, $\tilde{R}(T_0)=1$, by construction.

We can divide everything by $R(T_0)$ and obtain the "normalised" cost as

$$
\tilde{R}_{\alpha}(T) = \frac{R_{\alpha}(T)}{R(T_0)} =  \frac{R(T)}{R(T_0)} +\frac{\alpha}{R(T_0)} \left| T \right|
$$
The `cp` parameter in `rpart` is precisely the normalised value of `alpha`:

$$
\text{cp} = \frac{\alpha}{R(T_0)} 
$$
If we wanted to express the cost in the original scale, we can write:

$$
R_{\alpha}(T) = R(T) + \text{cp} \cdot R(T_0)  \cdot \left| T \right|
$$
:::

### Manually pruning the tree

Note that `rpart` use the default value of `cp`=0.01, which may not be optimal. We can use the `prune` function to prune the tree manually. 


For example, if we wanted to implement the 1 - SE rule, 

```{r}
# explore the table
cp_table <- credit_tree$cptable

# find the row with the minimum
min_row <- which.min(cp_table[, "xerror"])

# extract min error and SE of min error
min_xerror <- cp_table[min_row, "xerror"]
min_xstd   <- cp_table[min_row, "xstd"]

# compute the threshold: the best model according to the 
# 1 - SE rule is the first (simplest) within the threshold
threshold <- min_xerror + min_xstd

# find smallest tree within threshold
best_row <- which(cp_table[, "xerror"] <= threshold)[1]

best_cp <- cp_table[best_row, "CP"]

best_cp
```


We can now manually prune using:

```{r}
#| fig-height: 8
#| fig-width: 16
#| fig-align: center
#| echo: TRUE

# manually prune the tree
pruned_tree = prune(credit_tree, cp=best_cp)

# plot pruned tree
plot(pruned_tree)
text(pruned_tree, use.n = TRUE, cex=0.9)
```



