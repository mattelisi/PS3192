inner_panel = node_inner(p_m2, id = FALSE),
gp=gpar(fontsize=9))
#| fig-height: 6
#| fig-width: 12
#| fig-align: center
#| echo: TRUE
library(partykit)
p_m2 <- as.party(p_m)
plot(p_m2,
terminal_panel =  node_barplot(p_m2, gp = gpar(fontsize=8)),
inner_panel = node_inner(p_m2, id = FALSE),
gp=gpar(fontsize=8))
library(randomForest)
# Fit Random Forest model
rf_model <- randomForest(species ~ ., data = penguins, ntree = 500, importance = TRUE)
# Print summary
print(rf_model)
plot((rf_model))
varImpPlot(rf_model, main = "Feature Importance in Random Forest")
MDSplot(rf_model, penguins$species)
library(caret)
conf_mat <- confusionMatrix(rf_model$predicted, penguins$species)
# Plot confusion matrix
library(ggplot2)
ggplot(data = as.data.frame(conf_mat$table), aes(Prediction, Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "blue", high = "red") +
labs(title = "Confusion Matrix for Random Forest")
?varImpPlot
library(caret)
# compute predictions on training set
predictions <- predict(tree_model, penguins, type = "class")
# compute predictions on training set
predictions <- predict(p_m, penguins, type = "class")
# compute confusion matrix
conf_mat <- confusionMatrix(predictions, penguins$species)
conf_mat
print(conf_mat)
summary(conf_mat)
# fit tree
p_m <- rpart(species~ ., data = penguins)
# compute predictions on training set
predictions <- predict(p_m, penguins, type = "class")
# compute confusion matrix
library(caret)
conf_mat <- confusionMatrix(predictions, penguins$species)
summary(conf_mat)
(conf_mat)
library(rpart)
library(palmerpenguins)
library(ggplot2)
library(partykit)
library(dplyr)
library(caret)
results <- readRDS("../intro2ML/cali_results.RDS")
with(results, plot(MSE, test_MSE, pch=21, cex=0.2, col=rgb(0.4,0.4,0.4,0.5),
xlim=c(10, 90),
ylim=c(10, 90),
xlab="MSE train set (cross-validated)",
ylab="MSE test set"))
full_index <- which(nchar(results$formula)==max(nchar(results$formula)))
crossval_index <- which(results$MSE==min(results$MSE))
testset_index <- which(results$test_MSE==min(results$test_MSE))
points(results$MSE[crossval_index], results$test_MSE[crossval_index], pch=19, col="blue")
# points(results$MSE[full_index], results$test_MSE[full_index], pch=19, col="red")
# results$formula[full_index]
# results$formula[crossval_index]
# results$formula[testset_index]
# legend('topleft', col=c("blue","red"), pch=19, legend=c("selected by LOO cross-validation", "model with all 14 variables"), bty="n")
legend('topleft', col=c("blue"), pch=19, legend=c("selected by LOO cross-validation"), bty="n")
abline(a=0, b=1, lty=2)
# library(tidyverse)
#
# # Add a new column indicating whether the formula contains "rm"
# results <- results %>%
#   mutate(contains_rm = str_detect(formula, "\\brm\\b"))
#
# # Scatterplot of test_MSE vs. MSE, colored by presence of "rm"
# ggplot(results, aes(x = MSE, y = test_MSE, color = contains_rm)) +
#   geom_point(alpha = 0.6) +
#   labs(
#     title = "Test MSE vs MSE",
#     x = "MSE",
#     y = "Test MSE",
#     color = "Contains 'rm'"
#   ) +
#   theme_minimal()
#| fig-height: 4
#| fig-width: 10  # Half the slider width when default slide width is 4
#| fig-align: center
# intro to ML notes
library(rpart)
library(palmerpenguins)
library(partykit)
# tree example
p_m <- rpart(species~ ., data = penguins)
# # Prettier plot
# rpart.plot(
#   p_m,
#   type = 2,         # Split labels are drawn at the decision nodes
#   #extra = 104,      # Show predicted class and probability percentages
#   box.palette = "RdYlGn",  # Color nodes by prediction (red → yellow → green)
#   fallen.leaves = TRUE,    # Arrange terminal nodes at the same level
#   tweak = 1,     # Slightly increase text size for readability
#   shadow.col = "gray",  # Add shadows to the boxes
#   branch = 0.6    # Make branches less steep
# )
p_m2 <- as.party(p_m)
plot(p_m2, digits = 0, id = FALSE, terminal_panel = node_barplot(p_m2, id = FALSE, rot=0),
inner_panel = node_inner(p_m2, id = FALSE, pval = FALSE),
gp=gpar(fontsize=8,
height = 21))
#| fig-height: 4
#| fig-width: 10  # Half the slider width when default slide width is 4
#| fig-align: center
p_m2 <- as.party(p_m)
plot(p_m2, digits = 0, id = FALSE, terminal_panel = node_barplot(p_m2, id = FALSE, rot=0),
inner_panel = node_inner(p_m2, id = FALSE, pval = FALSE),
gp=gpar(fontsize=8,
height = 21))
#| fig-height: 5
#| fig-width: 10
#| fig-align: center
# Remove missing values
penguins <- na.omit(penguins)
# Fit a simple decision tree including island
p_m <- rpart(species ~ flipper_length_mm + bill_length_mm + island, data = penguins, method = "class")
# Convert to party object (optional, useful for tree visualization)
p_m2 <- as.party(p_m)
# Create a grid of values for flipper length, bill length, and include island
flipper_seq <- seq(min(penguins$flipper_length_mm)-10, max(penguins$flipper_length_mm)+10, length.out = 300)
bill_seq <- seq(min(penguins$bill_length_mm)-5, max(penguins$bill_length_mm)+5, length.out = 300)
# Expand grid for each island
grid <- expand.grid(
flipper_length_mm = flipper_seq,
bill_length_mm = bill_seq,
island = unique(penguins$island) # Include all island values
)
# Predict species for each grid point
grid$species <- predict(p_m, newdata = grid, type = "class")
grid$island2 <- ifelse(grid$island=="Biscoe", "Biscoe", "Dream OR Torgersen")
penguins$island2 <- ifelse(penguins$island=="Biscoe","Biscoe", "Dream OR Torgersen")
# Plot decision boundaries with facets for each island
ggplot() +
geom_tile(data = grid, aes(x = flipper_length_mm, y = bill_length_mm, fill = species), alpha = 0.8) +
geom_point(data = penguins, aes(x = flipper_length_mm, y = bill_length_mm, color = species), pch=21, size = 2) +
facet_wrap(~island2) +  # Separate plot for each island
labs(
x = "Flipper Length (mm)",
y = "Bill Length (mm)",
title = "Decision boundaries (split by island)"
) +
scale_fill_manual(values = c("Adelie" = "lightblue", "Chinstrap" = "lightgreen", "Gentoo" = "lightpink")) +
scale_color_manual(values = c("Adelie" = "blue", "Chinstrap" = "dark green", "Gentoo" = "red")) +
theme_minimal()+
coord_cartesian(xlim=range(penguins$flipper_length_mm),ylim=range(penguins$bill_length_mm))
#| fig-height: 6
#| fig-width: 13
#| fig-align: center
# Fit decision tree with deeper splits
p_m <- rpart(
species ~ .,
data = penguins,
method = "class",
cp = 0,        # No automatic pruning
minsplit = 2,  # Allow small splits
xval=50
)
p_m2 <- as.party(p_m)
plot(p_m2, digits = 0, id = FALSE,
terminal_panel =  node_barplot(p_m2, id = FALSE, rot=45, just="top",
ylines=0.25,
gp = gpar(fontsize=8)),
inner_panel = node_inner(p_m2, id = FALSE, pval = FALSE),
gp=gpar(fontsize=8,
height = 21))
#| fig-height: 6
#| fig-width: 6
#| fig-align: center
# Extract complexity parameter (CP) table
cp_data <- as.data.frame(p_m$cptable)
colnames(cp_data) <- c("cp", "nsplit", "rel_error", "xerror", "xstd")
# Compute number of terminal nodes
cp_data$size <- cp_data$nsplit + 1
cp_data$xstd <- cp_data$xstd
# cp_data
cp_data[,3:5] <- cp_data[,3:5] * 187/333
# Plot misclassification error vs. number of terminal nodes
ggplot(cp_data, aes(x = size)) +
geom_line(aes(y = rel_error, color = "Training"), linewidth = 1) +
geom_line(aes(y = xerror, color = "Cross-validation"), linewidth = 1) +
geom_errorbar(aes(y = xerror, ymin=xerror-xstd, ymax=xerror+xstd), color="blue", width=0.1, linewidth = 1) +
#geom_point(aes(y = xerror), size = 3, shape = 21, fill = "white") +
#geom_hline(aes(yintercept = min(xerror) + xstd[which.min(xerror)]), linetype = "dotted") +
geom_point(aes(x = size[which.min(xerror)], y = min(xerror)), color = "purple", size = 8, shape = 1) +
labs(
x = "Number of terminal nodes",
y = "Fraction of misclassifications",
) +
scale_color_manual(values = c("Training" = "red", "Cross-validation" = "blue"), name="") +
theme_minimal()
library(rpart)
p_m <- rpart(species~ ., data = penguins)
p_m
p_m <- rpart(species~ ., data = penguins,
minsplit = 4,  # minumun N in a node to attempt a split
xval=50)       # N. of cross-validations
p_m
#| fig-height: 6
#| fig-width: 6
#| fig-align: center
p_m <- rpart(
species ~ .,
data = penguins,
method = "class",
cp = 0,        # No automatic pruning
minsplit = 2,  # Allow small splits
xval=50
)
# Extract complexity parameter (CP) table
cp_data <- as.data.frame(p_m$cptable)
colnames(cp_data) <- c("cp", "nsplit", "rel_error", "xerror", "xstd")
# Compute number of terminal nodes
cp_data$size <- cp_data$nsplit + 1
cp_data$xstd <- cp_data$xstd
# cp_data
cp_data[,3:5] <- cp_data[,3:5] * 187/333
# Plot misclassification error vs. number of terminal nodes
ggplot(cp_data, aes(x = size)) +
geom_line(aes(y = rel_error, color = "Training"), linewidth = 1) +
geom_line(aes(y = xerror, color = "Cross-validation"), linewidth = 1) +
geom_errorbar(aes(y = xerror, ymin=xerror-xstd, ymax=xerror+xstd), color="blue", width=0.1, linewidth = 1) +
#geom_point(aes(y = xerror), size = 3, shape = 21, fill = "white") +
geom_hline(aes(yintercept = min(xerror) + xstd[which.min(xerror)]), linetype = "dotted") +
geom_point(aes(x = size[which.min(xerror)], y = min(xerror)), color = "purple", size = 8, shape = 1) +
labs(
x = "Number of terminal nodes",
y = "Fraction of misclassifications",
) +
scale_color_manual(values = c("Training" = "red", "Cross-validation" = "blue"), name="") +
theme_minimal()
#| fig-height: 6
#| fig-width: 13
#| fig-align: center
# Fit decision tree with deeper splits
p_m <- rpart(species~ ., data = penguins,
minsplit = 4,  # minumun N in a node to attempt a split
xval=50)       # N. of cross-validations
p_m2 <- as.party(p_m)
plot(p_m2, digits = 0, id = FALSE,
terminal_panel =  node_barplot(p_m2, id = FALSE, rot=45, just="top",
ylines=0.75,
gp = gpar(fontsize=8)),
inner_panel = node_inner(p_m2, id = FALSE, pval = FALSE),
gp=gpar(fontsize=12,
height = 11))
summary(p_m)
#| fig-height: 6
#| fig-width: 8
#| fig-align: center
#| echo: TRUE
par(xpd = TRUE)
plot(p_m, compress= TRUE)
text(p_m, use.n = TRUE, cex=0.8)
#| fig-height: 6
#| fig-width: 8
#| fig-align: center
#| echo: TRUE
library(rpart.plot)
rpart.plot(p_m)
#| fig-height: 6
#| fig-width: 12
#| fig-align: center
#| echo: TRUE
library(partykit)
p_m2 <- as.party(p_m)
plot(p_m2,
terminal_panel =  node_barplot(p_m2, gp = gpar(fontsize=8)),
inner_panel = node_inner(p_m2, id = FALSE),
gp=gpar(fontsize=8))
# fit tree
p_m <- rpart(species~ ., data = penguins)
# compute predictions on training set
predictions <- predict(p_m, penguins, type = "class")
# compute confusion matrix
library(caret)
conf_mat <- confusionMatrix(predictions, penguins$species)
print(conf_mat)
#| fig-height: 5
#| fig-width: 5
#| fig-align: center
#| echo: TRUE
#|
# Convert confusion matrix to a data frame for ggplot
conf_df <- as.data.frame(conf_mat$table)
# Create a heatmap plot
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +  # Add text labels
scale_fill_gradient(low = "blue", high = "red") +  # Color scale
labs(title = "Confusion Matrix for Decision Tree (rpart)",
x = "Predicted Class",
y = "Actual Class") +
theme_minimal()
#| fig-height: 5
#| fig-width: 6
#| fig-align: center
#| echo: TRUE
#|
# Convert confusion matrix to a data frame for ggplot
conf_df <- as.data.frame(conf_mat$table)
# Create a heatmap plot
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +  # Add text labels
scale_fill_gradient(low = "blue", high = "red") +  # Color scale
labs(title = "Confusion Matrix",
x = "Predicted Class",
y = "Actual Class") +
coord_equal()
theme_minimal()
#| fig-height: 5
#| fig-width: 5
#| fig-align: center
#| echo: TRUE
#|
# Convert confusion matrix to a data frame for ggplot
conf_df <- as.data.frame(conf_mat$table)
# Create a heatmap plot
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +  # Add text labels
scale_fill_gradient(low = "blue", high = "red") +  # Color scale
labs(title = "Confusion Matrix",
x = "Predicted Class",
y = "Actual Class") +
coord_equal()
theme_minimal()
#| fig-height: 5
#| fig-width: 5
#| fig-align: center
#| echo: TRUE
#|
# Convert confusion matrix to a data frame for ggplot
conf_df <- as.data.frame(conf_mat$table)
# Create a heatmap plot
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +  # Add text labels
scale_fill_gradient(low = "blue", high = "red") +  # Color scale
labs(title = "Confusion Matrix",
x = "Predicted Class",
y = "Actual Class") +
coord_equal() +
theme_minimal()
?randomForest
#| fig-height: 5
#| fig-width: 5
#| fig-align: center
#| echo: TRUE
ggplot(data = as.data.frame(conf_mat$table), aes(Prediction, Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "blue", high = "red") +
labs(title = "Confusion Matrix (out-of-bag predictions)")+
coord_equal() +
theme_minimal()
#| fig-height: 5
#| fig-width: 5
#| fig-align: center
#| echo: TRUE
ggplot(data = as.data.frame(conf_mat$table), aes(Prediction, Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "blue", high = "red") +
labs(title = "Confusion Matrix",
x = "Predicted Class",
y = "Actual Class") +
coord_equal() +
theme_minimal()
9/4
library(mvtnorm)
library(ellipse)
library(plot3D)
library(tidyverse)
library(MASS)
#| fig-height: 4
#| fig-width: 5
#| fig-align: center
par(bg = "#202A30", col.axis = "white", col.lab="white", mar=c(5,4,1.5,1)+0.1)
x <- seq(-5,7, length.out=300)
plot(x, dnorm(x, mean=1, sd=2), xlab="x",ylab="p(x)", type="l", col="white", lwd=3, bty="n",axes = FALSE)
axis(1, col = "white")
axis(2, col = "white", las = 1)
#| fig-height: 2.75
#| fig-width: 9
#| fig-align: center
n_obs <- 250
# Known covariance matrices
cov_matrices <- list(
"rho==-0.7" = matrix(c(1, -0.7, -0.7, 1), 2),
"rho==0" = matrix(c(1, 0, 0, 1), 2),
"rho==0.5" = matrix(c(1, 0.5, 0.5, 1), 2),
"rho==0.85" = matrix(c(1, 0.85, 0.85, 1), 2)
)
# Generate data
rn5 <- MASS::mvrnorm(n_obs, c(0, 0), Sigma = cov_matrices[["rho==-0.7"]])
r00 <- MASS::mvrnorm(n_obs, c(0, 0), Sigma = cov_matrices[["rho==0"]])
r03 <- MASS::mvrnorm(n_obs, c(0, 0), Sigma = cov_matrices[["rho==0.5"]])
r07 <- MASS::mvrnorm(n_obs, c(0, 0), Sigma = cov_matrices[["rho==0.85"]])
# Combine datasets
dat <- data.frame(rbind(rn5, r00, r03, r07))
colnames(dat) <- c("X1", "X2")
dat$correlation <- factor(c(rep("rho==-0.7", n_obs),
rep("rho==0", n_obs),
rep("rho==0.5", n_obs),
rep("rho==0.85", n_obs)),
levels = c("rho==-0.7", "rho==0", "rho==0.5", "rho==0.85"))
# Create theoretical ellipses
theoretical_ellipses <- do.call(rbind, lapply(names(cov_matrices), function(rho){
ell <- ellipse(cov_matrices[[rho]], level=0.95, npoints=100)
data.frame(X1=ell[,1], X2=ell[,2], correlation=rho)
}))
# Plot with theoretical ellipses and custom theme
ggplot(dat, aes(x = X1, y = X2)) +
geom_point(alpha = 0.3, color="white") +
geom_path(data=theoretical_ellipses, aes(x=X1, y=X2), color="white", size=1.2) +
facet_grid(. ~ correlation, labeller = label_parsed) +
labs(x = expression(x[1]), y = expression(x[2])) +
theme_minimal() +
theme(
panel.background = element_rect(fill="#202A30", color="#202A30"),
plot.background = element_rect(fill="#202A30", color="#202A30"),
strip.background = element_rect(fill="#202A30", color="#202A30"),
strip.text = element_text(color="white"),
axis.text = element_text(color="white"),
axis.title = element_text(color="white"),
panel.grid = element_blank()
)
library(mclust)
dat <- read.csv("mouse.csv")
head(dat)
#| fig-height: 4
#| fig-width: 5
#| fig-align: center
set.seed(42)
n_obs <- 300
# Define three Gaussian clusters (mouse shape)
ear1 <- MASS::mvrnorm(n = n_obs/2, mu = c(3, 8), Sigma = matrix(c(0.3, 0, 0, 0.3), 2))
ear2 <- MASS::mvrnorm(n_obs/2, c(8, 8), Sigma = matrix(c(0.3, 0, 0, 0.3), 2))
head <- MASS::mvrnorm(n_obs*2, c(5.5, 5), Sigma = matrix(c(2.5, 0, 0, 2.5), 2))
# Combine into one dataset
datC <- data.frame(rbind(ear1, ear2, head))
colnames(datC) <- c("X1", "X2")
# write_csv(dat,"mouse.csv")
datC$cluster <- factor(c(rep("Ear1", n_obs/2), rep("Ear2", n_obs/2), rep("Head", n_obs*2)))
# Plot dataset (similar to Wikipedia "mouse" example)
ggplot(datC, aes(x = X1, y = X2, color=cluster)) +
geom_point(alpha=0.8) +
theme_minimal() +
labs(x="X1", y="X2")
fit <- Mclust(dat)    # fit GMM
summary(fit)          # print summary
str(fit)
head(fit$z)
round(head(fit$z), digits=4)
#| fig-height: 4
#| fig-width: 5
#| fig-align: center
# Add predicted cluster to the dataset
dat$cluster_gmm <- factor(fit$classification)
# Plot GMM results
ggplot(dat,
aes(x = X1, y = X2,
color = cluster_gmm)) +
geom_point(alpha = 0.8) +
theme_minimal() +
labs(title = "GMM clustering",
x = "X1", y = "X2",
color = "cluster")
#| fig-height: 4
#| fig-width: 5
#| fig-align: center
# Perform k-means clustering (3 clusters)
kmeans_fit <- kmeans(dat, centers = 3)
# Add predicted cluster to the dataset
dat$cluster_kmeans <- factor(kmeans_fit$cluster)
# Plot k-means results
ggplot(dat, aes(x = X1, y = X2,
color = cluster_kmeans)) +
geom_point(alpha = 0.8) +
theme_minimal() +
labs(title = "K-means Clustering",
x = "X1", y = "X2",
color = "cluster")
fit$BIC
#| fig-height: 8
#| fig-width: 10
#| fig-align: center
plot(fit$BIC)
library(palmerpenguins)
str(penguins)
dat <- na.omit(penguins[, c("species", "bill_length_mm", "flipper_length_mm", "body_mass_g")])
fit <- Mclust(dat[,-1])
summary(fit)
summary(fit)
str(fit)
fit$parameters$pro
sum(fit$parameters$pro)
summary(fit)
setwd("/mnt/sda2/matteoHDD/git_local_HDD/PS3192/coursework")
