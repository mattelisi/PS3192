#| fig-width: 10  # Half the slider width when default slide width is 4
#| fig-align: center
p_m2 <- as.party(p_m)
plot(p_m2, digits = 0, id = FALSE, terminal_panel = node_barplot(p_m2, id = FALSE, rot=0),
inner_panel = node_inner(p_m2, id = FALSE, pval = FALSE),
gp=gpar(fontsize=8,
height = 21))
#| fig-height: 5
#| fig-width: 10
#| fig-align: center
# Remove missing values
penguins <- na.omit(penguins)
# Fit a simple decision tree including island
p_m <- rpart(species ~ flipper_length_mm + bill_length_mm + island, data = penguins, method = "class")
# Convert to party object (optional, useful for tree visualization)
p_m2 <- as.party(p_m)
# Create a grid of values for flipper length, bill length, and include island
flipper_seq <- seq(min(penguins$flipper_length_mm)-10, max(penguins$flipper_length_mm)+10, length.out = 300)
bill_seq <- seq(min(penguins$bill_length_mm)-5, max(penguins$bill_length_mm)+5, length.out = 300)
# Expand grid for each island
grid <- expand.grid(
flipper_length_mm = flipper_seq,
bill_length_mm = bill_seq,
island = unique(penguins$island) # Include all island values
)
# Predict species for each grid point
grid$species <- predict(p_m, newdata = grid, type = "class")
grid$island2 <- ifelse(grid$island=="Biscoe", "Biscoe", "Dream OR Torgersen")
penguins$island2 <- ifelse(penguins$island=="Biscoe","Biscoe", "Dream OR Torgersen")
# Plot decision boundaries with facets for each island
ggplot() +
geom_tile(data = grid, aes(x = flipper_length_mm, y = bill_length_mm, fill = species), alpha = 0.8) +
geom_point(data = penguins, aes(x = flipper_length_mm, y = bill_length_mm, color = species), pch=21, size = 2) +
facet_wrap(~island2) +  # Separate plot for each island
labs(
x = "Flipper Length (mm)",
y = "Bill Length (mm)",
title = "Decision boundaries (split by island)"
) +
scale_fill_manual(values = c("Adelie" = "lightblue", "Chinstrap" = "lightgreen", "Gentoo" = "lightpink")) +
scale_color_manual(values = c("Adelie" = "blue", "Chinstrap" = "dark green", "Gentoo" = "red")) +
theme_minimal()+
coord_cartesian(xlim=range(penguins$flipper_length_mm),ylim=range(penguins$bill_length_mm))
#| fig-height: 6
#| fig-width: 13
#| fig-align: center
# Fit decision tree with deeper splits
p_m <- rpart(
species ~ .,
data = penguins,
method = "class",
cp = 0,        # No automatic pruning
minsplit = 2,  # Allow small splits
xval=50
)
p_m2 <- as.party(p_m)
plot(p_m2, digits = 0, id = FALSE,
terminal_panel =  node_barplot(p_m2, id = FALSE, rot=45, just="top",
ylines=0.25,
gp = gpar(fontsize=8)),
inner_panel = node_inner(p_m2, id = FALSE, pval = FALSE),
gp=gpar(fontsize=8,
height = 21))
#| fig-height: 6
#| fig-width: 6
#| fig-align: center
# Extract complexity parameter (CP) table
cp_data <- as.data.frame(p_m$cptable)
colnames(cp_data) <- c("cp", "nsplit", "rel_error", "xerror", "xstd")
# Compute number of terminal nodes
cp_data$size <- cp_data$nsplit + 1
cp_data$xstd <- cp_data$xstd
# cp_data
cp_data[,3:5] <- cp_data[,3:5] * 187/333
# Plot misclassification error vs. number of terminal nodes
ggplot(cp_data, aes(x = size)) +
geom_line(aes(y = rel_error, color = "Training"), linewidth = 1) +
geom_line(aes(y = xerror, color = "Cross-validation"), linewidth = 1) +
geom_errorbar(aes(y = xerror, ymin=xerror-xstd, ymax=xerror+xstd), color="blue", width=0.1, linewidth = 1) +
#geom_point(aes(y = xerror), size = 3, shape = 21, fill = "white") +
#geom_hline(aes(yintercept = min(xerror) + xstd[which.min(xerror)]), linetype = "dotted") +
geom_point(aes(x = size[which.min(xerror)], y = min(xerror)), color = "purple", size = 8, shape = 1) +
labs(
x = "Number of terminal nodes",
y = "Fraction of misclassifications",
) +
scale_color_manual(values = c("Training" = "red", "Cross-validation" = "blue"), name="") +
theme_minimal()
library(rpart)
p_m <- rpart(species~ ., data = penguins)
p_m
p_m <- rpart(species~ ., data = penguins,
minsplit = 4,  # minumun N in a node to attempt a split
xval=50)       # N. of cross-validations
p_m
#| fig-height: 6
#| fig-width: 6
#| fig-align: center
p_m <- rpart(
species ~ .,
data = penguins,
method = "class",
cp = 0,        # No automatic pruning
minsplit = 2,  # Allow small splits
xval=50
)
# Extract complexity parameter (CP) table
cp_data <- as.data.frame(p_m$cptable)
colnames(cp_data) <- c("cp", "nsplit", "rel_error", "xerror", "xstd")
# Compute number of terminal nodes
cp_data$size <- cp_data$nsplit + 1
cp_data$xstd <- cp_data$xstd
# cp_data
cp_data[,3:5] <- cp_data[,3:5] * 187/333
# Plot misclassification error vs. number of terminal nodes
ggplot(cp_data, aes(x = size)) +
geom_line(aes(y = rel_error, color = "Training"), linewidth = 1) +
geom_line(aes(y = xerror, color = "Cross-validation"), linewidth = 1) +
geom_errorbar(aes(y = xerror, ymin=xerror-xstd, ymax=xerror+xstd), color="blue", width=0.1, linewidth = 1) +
#geom_point(aes(y = xerror), size = 3, shape = 21, fill = "white") +
geom_hline(aes(yintercept = min(xerror) + xstd[which.min(xerror)]), linetype = "dotted") +
geom_point(aes(x = size[which.min(xerror)], y = min(xerror)), color = "purple", size = 8, shape = 1) +
labs(
x = "Number of terminal nodes",
y = "Fraction of misclassifications",
) +
scale_color_manual(values = c("Training" = "red", "Cross-validation" = "blue"), name="") +
theme_minimal()
#| fig-height: 6
#| fig-width: 13
#| fig-align: center
# Fit decision tree with deeper splits
p_m <- rpart(species~ ., data = penguins,
minsplit = 4,  # minumun N in a node to attempt a split
xval=50)       # N. of cross-validations
p_m2 <- as.party(p_m)
plot(p_m2, digits = 0, id = FALSE,
terminal_panel =  node_barplot(p_m2, id = FALSE, rot=45, just="top",
ylines=0.75,
gp = gpar(fontsize=8)),
inner_panel = node_inner(p_m2, id = FALSE, pval = FALSE),
gp=gpar(fontsize=12,
height = 11))
summary(p_m)
#| fig-height: 6
#| fig-width: 8
#| fig-align: center
#| echo: TRUE
par(xpd = TRUE)
plot(p_m, compress= TRUE)
text(p_m, use.n = TRUE, cex=0.8)
#| fig-height: 6
#| fig-width: 8
#| fig-align: center
#| echo: TRUE
library(rpart.plot)
rpart.plot(p_m)
#| fig-height: 6
#| fig-width: 12
#| fig-align: center
#| echo: TRUE
library(partykit)
p_m2 <- as.party(p_m)
plot(p_m2,
terminal_panel =  node_barplot(p_m2, gp = gpar(fontsize=8)),
inner_panel = node_inner(p_m2, id = FALSE),
gp=gpar(fontsize=8))
# fit tree
p_m <- rpart(species~ ., data = penguins)
# compute predictions on training set
predictions <- predict(p_m, penguins, type = "class")
# compute confusion matrix
library(caret)
conf_mat <- confusionMatrix(predictions, penguins$species)
print(conf_mat)
#| fig-height: 5
#| fig-width: 5
#| fig-align: center
#| echo: TRUE
#|
# Convert confusion matrix to a data frame for ggplot
conf_df <- as.data.frame(conf_mat$table)
# Create a heatmap plot
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +  # Add text labels
scale_fill_gradient(low = "blue", high = "red") +  # Color scale
labs(title = "Confusion Matrix for Decision Tree (rpart)",
x = "Predicted Class",
y = "Actual Class") +
theme_minimal()
#| fig-height: 5
#| fig-width: 6
#| fig-align: center
#| echo: TRUE
#|
# Convert confusion matrix to a data frame for ggplot
conf_df <- as.data.frame(conf_mat$table)
# Create a heatmap plot
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +  # Add text labels
scale_fill_gradient(low = "blue", high = "red") +  # Color scale
labs(title = "Confusion Matrix",
x = "Predicted Class",
y = "Actual Class") +
coord_equal()
theme_minimal()
#| fig-height: 5
#| fig-width: 5
#| fig-align: center
#| echo: TRUE
#|
# Convert confusion matrix to a data frame for ggplot
conf_df <- as.data.frame(conf_mat$table)
# Create a heatmap plot
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +  # Add text labels
scale_fill_gradient(low = "blue", high = "red") +  # Color scale
labs(title = "Confusion Matrix",
x = "Predicted Class",
y = "Actual Class") +
coord_equal()
theme_minimal()
#| fig-height: 5
#| fig-width: 5
#| fig-align: center
#| echo: TRUE
#|
# Convert confusion matrix to a data frame for ggplot
conf_df <- as.data.frame(conf_mat$table)
# Create a heatmap plot
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +  # Add text labels
scale_fill_gradient(low = "blue", high = "red") +  # Color scale
labs(title = "Confusion Matrix",
x = "Predicted Class",
y = "Actual Class") +
coord_equal() +
theme_minimal()
?randomForest
#| fig-height: 5
#| fig-width: 5
#| fig-align: center
#| echo: TRUE
ggplot(data = as.data.frame(conf_mat$table), aes(Prediction, Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "blue", high = "red") +
labs(title = "Confusion Matrix (out-of-bag predictions)")+
coord_equal() +
theme_minimal()
#| fig-height: 5
#| fig-width: 5
#| fig-align: center
#| echo: TRUE
ggplot(data = as.data.frame(conf_mat$table), aes(Prediction, Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), color = "white", size = 5) +
scale_fill_gradient(low = "blue", high = "red") +
labs(title = "Confusion Matrix",
x = "Predicted Class",
y = "Actual Class") +
coord_equal() +
theme_minimal()
9/4
library(mvtnorm)
library(ellipse)
library(plot3D)
library(tidyverse)
library(MASS)
#| fig-height: 4
#| fig-width: 5
#| fig-align: center
par(bg = "#202A30", col.axis = "white", col.lab="white", mar=c(5,4,1.5,1)+0.1)
x <- seq(-5,7, length.out=300)
plot(x, dnorm(x, mean=1, sd=2), xlab="x",ylab="p(x)", type="l", col="white", lwd=3, bty="n",axes = FALSE)
axis(1, col = "white")
axis(2, col = "white", las = 1)
#| fig-height: 2.75
#| fig-width: 9
#| fig-align: center
n_obs <- 250
# Known covariance matrices
cov_matrices <- list(
"rho==-0.7" = matrix(c(1, -0.7, -0.7, 1), 2),
"rho==0" = matrix(c(1, 0, 0, 1), 2),
"rho==0.5" = matrix(c(1, 0.5, 0.5, 1), 2),
"rho==0.85" = matrix(c(1, 0.85, 0.85, 1), 2)
)
# Generate data
rn5 <- MASS::mvrnorm(n_obs, c(0, 0), Sigma = cov_matrices[["rho==-0.7"]])
r00 <- MASS::mvrnorm(n_obs, c(0, 0), Sigma = cov_matrices[["rho==0"]])
r03 <- MASS::mvrnorm(n_obs, c(0, 0), Sigma = cov_matrices[["rho==0.5"]])
r07 <- MASS::mvrnorm(n_obs, c(0, 0), Sigma = cov_matrices[["rho==0.85"]])
# Combine datasets
dat <- data.frame(rbind(rn5, r00, r03, r07))
colnames(dat) <- c("X1", "X2")
dat$correlation <- factor(c(rep("rho==-0.7", n_obs),
rep("rho==0", n_obs),
rep("rho==0.5", n_obs),
rep("rho==0.85", n_obs)),
levels = c("rho==-0.7", "rho==0", "rho==0.5", "rho==0.85"))
# Create theoretical ellipses
theoretical_ellipses <- do.call(rbind, lapply(names(cov_matrices), function(rho){
ell <- ellipse(cov_matrices[[rho]], level=0.95, npoints=100)
data.frame(X1=ell[,1], X2=ell[,2], correlation=rho)
}))
# Plot with theoretical ellipses and custom theme
ggplot(dat, aes(x = X1, y = X2)) +
geom_point(alpha = 0.3, color="white") +
geom_path(data=theoretical_ellipses, aes(x=X1, y=X2), color="white", size=1.2) +
facet_grid(. ~ correlation, labeller = label_parsed) +
labs(x = expression(x[1]), y = expression(x[2])) +
theme_minimal() +
theme(
panel.background = element_rect(fill="#202A30", color="#202A30"),
plot.background = element_rect(fill="#202A30", color="#202A30"),
strip.background = element_rect(fill="#202A30", color="#202A30"),
strip.text = element_text(color="white"),
axis.text = element_text(color="white"),
axis.title = element_text(color="white"),
panel.grid = element_blank()
)
library(mclust)
dat <- read.csv("mouse.csv")
head(dat)
#| fig-height: 4
#| fig-width: 5
#| fig-align: center
set.seed(42)
n_obs <- 300
# Define three Gaussian clusters (mouse shape)
ear1 <- MASS::mvrnorm(n = n_obs/2, mu = c(3, 8), Sigma = matrix(c(0.3, 0, 0, 0.3), 2))
ear2 <- MASS::mvrnorm(n_obs/2, c(8, 8), Sigma = matrix(c(0.3, 0, 0, 0.3), 2))
head <- MASS::mvrnorm(n_obs*2, c(5.5, 5), Sigma = matrix(c(2.5, 0, 0, 2.5), 2))
# Combine into one dataset
datC <- data.frame(rbind(ear1, ear2, head))
colnames(datC) <- c("X1", "X2")
# write_csv(dat,"mouse.csv")
datC$cluster <- factor(c(rep("Ear1", n_obs/2), rep("Ear2", n_obs/2), rep("Head", n_obs*2)))
# Plot dataset (similar to Wikipedia "mouse" example)
ggplot(datC, aes(x = X1, y = X2, color=cluster)) +
geom_point(alpha=0.8) +
theme_minimal() +
labs(x="X1", y="X2")
fit <- Mclust(dat)    # fit GMM
summary(fit)          # print summary
str(fit)
head(fit$z)
round(head(fit$z), digits=4)
#| fig-height: 4
#| fig-width: 5
#| fig-align: center
# Add predicted cluster to the dataset
dat$cluster_gmm <- factor(fit$classification)
# Plot GMM results
ggplot(dat,
aes(x = X1, y = X2,
color = cluster_gmm)) +
geom_point(alpha = 0.8) +
theme_minimal() +
labs(title = "GMM clustering",
x = "X1", y = "X2",
color = "cluster")
#| fig-height: 4
#| fig-width: 5
#| fig-align: center
# Perform k-means clustering (3 clusters)
kmeans_fit <- kmeans(dat, centers = 3)
# Add predicted cluster to the dataset
dat$cluster_kmeans <- factor(kmeans_fit$cluster)
# Plot k-means results
ggplot(dat, aes(x = X1, y = X2,
color = cluster_kmeans)) +
geom_point(alpha = 0.8) +
theme_minimal() +
labs(title = "K-means Clustering",
x = "X1", y = "X2",
color = "cluster")
fit$BIC
#| fig-height: 8
#| fig-width: 10
#| fig-align: center
plot(fit$BIC)
library(palmerpenguins)
str(penguins)
dat <- na.omit(penguins[, c("species", "bill_length_mm", "flipper_length_mm", "body_mass_g")])
fit <- Mclust(dat[,-1])
summary(fit)
summary(fit)
str(fit)
fit$parameters$pro
sum(fit$parameters$pro)
summary(fit)
library(brms)
library(bayestestR)
library(ggplot2)
# 1. Simulate data ---------------------------------------------
set.seed(42)
N <- 300
age <- rnorm(N, mean = 70, sd = 10)  # predictor
# Path a: mediator model (linear)
a <- 0.02
acuity <- 0.1 + a * age + rnorm(N, mean = 0, sd = 0.05)
# Path b and c': outcome model (negative binomial)
b <- 1.5       # effect of acuity on hallucinations (on log scale)
c_prime <- 0.01  # direct effect of age on hallucinations
lp <- b * acuity + c_prime * age
mu <- exp(lp)  # mean of NB
theta <- 2  # dispersion parameter for NB
# Negative binomial sampling
p <- theta / (theta + mu)
total_complex <- rnbinom(N, size = theta, prob = p)
# Create dataframe
dat <- data.frame(
Age = age,
LogMAR_Acuity = acuity,
Total_Complex = total_complex
)
# 2. Fit Bayesian mediation models ------------------------------
# Mediator model
model_m <- brm(
formula = LogMAR_Acuity ~ Age,
data = dat,
family = gaussian(),
chains = 4, cores = 4, seed = 123
)
# Outcome model
model_y <- brm(
formula = Total_Complex ~ Age + LogMAR_Acuity,
data = dat,
family = negbinomial(),
chains = 4, cores = 4, seed = 123
)
post_m <- posterior_samples(model_m)
post_y <- posterior_samples(model_y)
a_post <- post_m$b_Age
b_post <- post_y$b_LogMAR_Acuity
c_post <- post_y$b_Age
indirect <- a_post * b_post
direct <- c_post
total <- indirect + direct
posterior_effects <- data.frame(
indirect = indirect,
direct = direct,
total = total
)
describe_posterior(posterior_effects)
library(reshape2)
posterior_long <- melt(posterior_effects)
ggplot(posterior_long, aes(x = value)) +
geom_density(fill = "skyblue", alpha = 0.6) +
facet_wrap(~ variable, scales = "free") +
theme_minimal() +
labs(title = "Posterior Distributions of Mediation Effects",
x = "Effect size", y = "Density")
# Step 1: Create two values of Age for comparison (x and x')
x <- 60
x_prime <- 80
# Step 2: Create new data frames to simulate mediator values
newdata_x <- data.frame(Age = rep(x, 1000))
newdata_xp <- data.frame(Age = rep(x_prime, 1000))
# Step 3: Simulate mediator values (LogMAR_Acuity) under both x and x'
# These are posterior predictive draws
m_x <- posterior_predict(model_m, newdata = newdata_x)
m_xp <- posterior_predict(model_m, newdata = newdata_xp)
# Take one draw per posterior sample (e.g., 1000 samples)
# Then, simulate outcomes using these predicted mediators
# Step 4: Create outcome model datasets
make_data_y <- function(age_val, mediator_vals) {
data.frame(
Age = rep(age_val, length(mediator_vals)),
LogMAR_Acuity = mediator_vals
)
}
# Y(x', M(x)): Age=x', mediator as if Age=x
dat_NIE <- make_data_y(x_prime, apply(m_x, 1, mean))
# Y(x', M(x')): Age=x', mediator as if Age=x'
dat_natural <- make_data_y(x_prime, apply(m_xp, 1, mean))
# Y(x, M(x)): Age=x, mediator as if Age=x
dat_baseline <- make_data_y(x, apply(m_x, 1, mean))
# Step 5: Simulate outcomes from the outcome model (posterior predictive)
y_natural <- posterior_predict(model_y, newdata = dat_natural)
y_nie <- posterior_predict(model_y, newdata = dat_NIE)
y_baseline <- posterior_predict(model_y, newdata = dat_baseline)
# Step 6: Compute contrasts (average over posterior draws)
TE <- rowMeans(y_natural) - rowMeans(y_baseline)
NIE <- rowMeans(y_natural) - rowMeans(y_nie)
NDE <- rowMeans(y_nie) - rowMeans(y_baseline)
# Step 7: Summarize with bayestestR
library(bayestestR)
describe_posterior(data.frame(
total_effect = TE,
indirect_effect = NIE,
direct_effect = NDE
))
rm(list=ls())
library(tidyverse)
library(jpeg)
hablar::set_wd_to_script_path()
# load info
d <- read_csv("datasets/gazedata.csv")
img <- readJPEG("afraid.jpg")
# make plot
ggplot(d, aes(x = x, y = y)) +
annotation_raster(img, xmin = 0, xmax = 1, ymin = 0, ymax = 1) +
geom_point(pch=21)
37 * 6000
37 * 25
